name: Deploy to GCP Cloud Run

on:
  push:
    branches:
      - main
  workflow_dispatch: # Allows manual trigger

env:
  PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  REGION: us-central1
  FRONTEND_SERVICE: studybuddy-frontend
  AI_SERVICE: studybuddy-ai-service

jobs:
  setup-database:
    name: Setup Database (pgvector extension)
    runs-on: ubuntu-latest

    steps:
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Install PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: Enable pgvector extension
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          echo "Enabling pgvector extension on Cloud SQL database..."
          # Try to connect and enable extension
          # If connection uses Unix socket or Cloud SQL proxy, psql may need different flags
          psql "${{ secrets.DATABASE_URL }}" -c "CREATE EXTENSION IF NOT EXISTS vector;" || \
            (echo "Warning: Could not enable extension. This may be normal if:" && \
             echo "  - Extension already exists" && \
             echo "  - Connection uses Cloud SQL proxy (extension must be enabled via Cloud Console)" && \
             echo "  - Database permissions require manual setup")

  deploy-frontend:
    name: Deploy Frontend to Cloud Run
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Configure Docker for GCR
        run: gcloud auth configure-docker

      - name: Build Frontend Docker image
        run: |
          docker build -t gcr.io/$PROJECT_ID/$FRONTEND_SERVICE:${{ github.sha }} \
            -t gcr.io/$PROJECT_ID/$FRONTEND_SERVICE:latest \
            -f frontend/Dockerfile ./frontend

      - name: Push Frontend image to GCR
        run: |
          docker push gcr.io/$PROJECT_ID/$FRONTEND_SERVICE:${{ github.sha }}
          docker push gcr.io/$PROJECT_ID/$FRONTEND_SERVICE:latest

      - name: Deploy Frontend to Cloud Run
        env:
          CLOUD_SQL_CONNECTION_NAME: ${{ secrets.CLOUD_SQL_CONNECTION_NAME }}
          DATABASE_URL_SECRET: ${{ secrets.DATABASE_URL }}
        run: |
          # Build DATABASE_URL for Unix socket connection if CLOUD_SQL_CONNECTION_NAME is provided
          if [ -n "$CLOUD_SQL_CONNECTION_NAME" ]; then
            # Extract credentials from DATABASE_URL secret
            # Format: postgresql://username:password@host:port/database
            DB_USER=$(echo "$DATABASE_URL_SECRET" | sed -n 's|postgresql://\([^:]*\):.*|\1|p')
            DB_PASS=$(echo "$DATABASE_URL_SECRET" | sed -n 's|postgresql://[^:]*:\([^@]*\)@.*|\1|p')
            DB_NAME=$(echo "$DATABASE_URL_SECRET" | sed -n 's|.*/\([^?]*\).*|\1|p')

            # Unix socket format: postgresql://user:password@localhost/database?host=/cloudsql/CONNECTION_NAME
            DATABASE_URL="postgresql://${DB_USER}:${DB_PASS}@localhost/${DB_NAME}?host=/cloudsql/$CLOUD_SQL_CONNECTION_NAME"
            CLOUD_SQL_FLAG="--add-cloudsql-instances=$CLOUD_SQL_CONNECTION_NAME"
            echo "Using Unix socket connection via Cloud SQL Proxy"
          else
            # Fall back to direct IP connection (requires authorized networks)
            DATABASE_URL="$DATABASE_URL_SECRET"
            CLOUD_SQL_FLAG=""
            echo "Using direct IP connection (ensure IP is authorized)"
          fi
          
          gcloud run deploy $FRONTEND_SERVICE \
            --image gcr.io/$PROJECT_ID/$FRONTEND_SERVICE:${{ github.sha }} \
            --region $REGION \
            --platform managed \
            --allow-unauthenticated \
            $CLOUD_SQL_FLAG \
            --set-env-vars "DATABASE_URL=$DATABASE_URL" \
            --set-env-vars "NEXTAUTH_SECRET=${{ secrets.NEXTAUTH_SECRET }}" \
            --set-env-vars "NEXTAUTH_URL=${{ secrets.NEXTAUTH_URL }}" \
            --set-env-vars "AI_SERVICE_URL=${{ secrets.AI_SERVICE_URL }}" \
            --set-env-vars "NODE_ENV=production" \
            --set-env-vars "GCS_PROJECT_ID=$PROJECT_ID" \
            --set-env-vars "GCS_BUCKET=${{ secrets.GCS_BUCKET_NAME }}" \
            --set-env-vars "CLOUD_TASKS_LOCATION=$REGION" \
            --set-env-vars "CLOUD_TASKS_QUEUE=studybuddy-jobs" \
            --set-env-vars "GOOGLE_OAUTH_CLIENT_ID=${{ secrets.GOOGLE_OAUTH_CLIENT_ID }}" \
            --set-env-vars "GOOGLE_OAUTH_CLIENT_SECRET=${{ secrets.GOOGLE_OAUTH_CLIENT_SECRET }}" \
            --memory 1Gi \
            --cpu 1 \
            --timeout 300 \
            --max-instances 10 \
            --min-instances 0

  deploy-ai-service:
    name: Deploy AI Service to Cloud Run
    runs-on: ubuntu-latest
    needs: [setup-database]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Configure Docker for GCR
        run: gcloud auth configure-docker

      - name: Build AI Service Docker image
        run: |
          docker build -t gcr.io/$PROJECT_ID/$AI_SERVICE:${{ github.sha }} \
            -t gcr.io/$PROJECT_ID/$AI_SERVICE:latest \
            -f ai-service/Dockerfile ./ai-service

      - name: Push AI Service image to GCR
        run: |
          docker push gcr.io/$PROJECT_ID/$AI_SERVICE:${{ github.sha }}
          docker push gcr.io/$PROJECT_ID/$AI_SERVICE:latest

      - name: Deploy AI Service to Cloud Run
        env:
          LLM_PROVIDER: ${{ secrets.LLM_PROVIDER != '' && secrets.LLM_PROVIDER || 'openai' }}
          OPENAI_MODEL: ${{ secrets.OPENAI_MODEL != '' && secrets.OPENAI_MODEL || 'gpt-5' }}
          OPENAI_MINI_MODEL: ${{ secrets.OPENAI_MINI_MODEL != '' && secrets.OPENAI_MINI_MODEL || 'gpt-5-mini' }}
          OPENAI_EMBEDDING_MODEL: ${{ secrets.OPENAI_EMBEDDING_MODEL != '' && secrets.OPENAI_EMBEDDING_MODEL || 'text-embedding-3-small' }}
          CLOUD_SQL_CONNECTION_NAME: ${{ secrets.CLOUD_SQL_CONNECTION_NAME }}
          DATABASE_URL_SECRET: ${{ secrets.DATABASE_URL }}
        run: |
          # Build DATABASE_URL for Unix socket connection if CLOUD_SQL_CONNECTION_NAME is provided
          # Note: Python asyncpg requires format without @localhost (just @/)
          if [ -n "$CLOUD_SQL_CONNECTION_NAME" ]; then
            # Extract credentials from DATABASE_URL secret
            # Format: postgresql://username:password@host:port/database
            DB_USER=$(echo "$DATABASE_URL_SECRET" | sed -n 's|postgresql://\([^:]*\):.*|\1|p')
            DB_PASS=$(echo "$DATABASE_URL_SECRET" | sed -n 's|postgresql://[^:]*:\([^@]*\)@.*|\1|p')
            DB_NAME=$(echo "$DATABASE_URL_SECRET" | sed -n 's|.*/\([^?]*\).*|\1|p')

            # Unix socket format for Python asyncpg: postgresql://user:password@/database?host=/cloudsql/CONNECTION_NAME
            DATABASE_URL="postgresql://${DB_USER}:${DB_PASS}@/${DB_NAME}?host=/cloudsql/$CLOUD_SQL_CONNECTION_NAME"
            CLOUD_SQL_FLAG="--add-cloudsql-instances=$CLOUD_SQL_CONNECTION_NAME"
            echo "Using Unix socket connection via Cloud SQL Proxy (asyncpg format)"
          else
            # Fall back to direct IP connection (requires authorized networks)
            DATABASE_URL="$DATABASE_URL_SECRET"
            CLOUD_SQL_FLAG=""
            echo "Using direct IP connection (ensure IP is authorized)"
          fi

          gcloud run deploy $AI_SERVICE \
            --image gcr.io/$PROJECT_ID/$AI_SERVICE:${{ github.sha }} \
            --region $REGION \
            --platform managed \
            --allow-unauthenticated \
            $CLOUD_SQL_FLAG \
            --set-env-vars "DATABASE_URL=$DATABASE_URL" \
            --set-env-vars "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}" \
            --set-env-vars "ENVIRONMENT=production" \
            --set-env-vars "LLM_PROVIDER=${LLM_PROVIDER}" \
            --set-env-vars "OPENAI_MODEL=${OPENAI_MODEL}" \
            --set-env-vars "OPENAI_MINI_MODEL=${OPENAI_MINI_MODEL}" \
            --set-env-vars "OPENAI_EMBEDDING_MODEL=${OPENAI_EMBEDDING_MODEL}" \
            --set-env-vars "GCS_BUCKET_NAME=${{ secrets.GCS_BUCKET_NAME }}" \
            --set-env-vars "GCS_BUCKET=${{ secrets.GCS_BUCKET_NAME }}" \
            --set-env-vars "FRONTEND_URL=${{ secrets.NEXTAUTH_URL }}" \
            --set-env-vars "AI_SERVICE_URL=${{ secrets.AI_SERVICE_URL }}" \
            --memory 2Gi \
            --cpu 2 \
            --timeout 600 \
            --max-instances 10 \
            --min-instances 0

  run-migrations:
    name: Run Database Migrations
    runs-on: ubuntu-latest
    needs: [setup-database]
    # Run migrations after database setup
    # Services should handle missing tables gracefully or have health checks

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: |
          cd frontend
          npm install --legacy-peer-deps

      - name: Install netcat for port checking
        run: |
          sudo apt-get update
          sudo apt-get install -y netcat-openbsd

      - name: Start Cloud SQL Proxy (if connection name provided)
        env:
          CLOUD_SQL_CONNECTION_NAME: ${{ secrets.CLOUD_SQL_CONNECTION_NAME }}
        run: |
          # Check if connection name is provided
          if [ -z "$CLOUD_SQL_CONNECTION_NAME" ]; then
            echo "CLOUD_SQL_CONNECTION_NAME not provided, skipping proxy setup"
            echo "Will attempt direct database connection (ensure IP is authorized)"
            exit 0
          fi
          
          echo "Starting Cloud SQL Proxy for: $CLOUD_SQL_CONNECTION_NAME"
          # Download Cloud SQL Proxy v2
          wget -q https://storage.googleapis.com/cloud-sql-connectors/cloud-sql-proxy/v2.8.0/cloud-sql-proxy.linux.amd64 -O cloud-sql-proxy
          chmod +x cloud-sql-proxy
          
          # Start proxy in background on port 5432
          ./cloud-sql-proxy "$CLOUD_SQL_CONNECTION_NAME" --port 5432 > /tmp/cloud-sql-proxy.log 2>&1 &
          PROXY_PID=$!
          echo "PROXY_PID=$PROXY_PID" >> $GITHUB_ENV
          
          # Wait for proxy to be ready (check if port is listening)
          echo "Waiting for Cloud SQL Proxy to start..."
          for i in {1..30}; do
            if nc -z localhost 5432; then
              echo "Cloud SQL Proxy started successfully on port 5432"
              exit 0
            fi
            sleep 1
          done
          
          if ! nc -z localhost 5432; then
            echo "Warning: Cloud SQL Proxy may not have started. Continuing anyway..."
            cat /tmp/cloud-sql-proxy.log || true
          fi

      - name: Push Prisma schema to database
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          CLOUD_SQL_CONNECTION_NAME: ${{ secrets.CLOUD_SQL_CONNECTION_NAME }}
        run: |
          cd frontend
          # If Cloud SQL Proxy is used, replace host in DATABASE_URL with localhost
          if [ -n "$CLOUD_SQL_CONNECTION_NAME" ]; then
            export DATABASE_URL=$(echo "$DATABASE_URL" | sed 's/@[^:]*:/@localhost:/')
            echo "Using DATABASE_URL via Cloud SQL Proxy"
          else
            echo "Using DATABASE_URL directly (ensure IP is authorized)"
          fi
          echo "Pushing Prisma schema to database..."
          npx prisma db push --accept-data-loss --skip-generate

      - name: Generate Prisma Client
        run: |
          cd frontend
          npx prisma generate

      - name: Create vector index (if not exists)
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          CLOUD_SQL_CONNECTION_NAME: ${{ secrets.CLOUD_SQL_CONNECTION_NAME }}
        run: |
          # Install PostgreSQL client
          sudo apt-get update
          sudo apt-get install -y postgresql-client
          
          # Update DATABASE_URL if using Cloud SQL Proxy
          if [ -n "$CLOUD_SQL_CONNECTION_NAME" ]; then
            export DATABASE_URL=$(echo "$DATABASE_URL" | sed 's/@[^:]*:/@localhost:/')
            echo "Using DATABASE_URL via Cloud SQL Proxy for index creation"
          fi
          
          # Create vector index for embeddings (HNSW for performance)
          # This will fail silently if index already exists, which is fine
          echo "Creating vector index for material_chunks.chunk_embedding..."
          psql "$DATABASE_URL" <<EOF || echo "Index creation skipped (may already exist)"
          CREATE INDEX IF NOT EXISTS material_chunks_embedding_hnsw_idx 
          ON material_chunks 
          USING hnsw (chunk_embedding vector_cosine_ops)
          WITH (m = 16, ef_construction = 64);
          EOF
